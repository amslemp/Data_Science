{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4428428d",
   "metadata": {},
   "source": [
    "# K-Prototype For Enrollment\n",
    "\n",
    "This model deployment is for the marketing team to see not only where we should target for geofencing, but also with what. Given the sheer number of programs, it is good for us to know who the students are, where they live, their age, etc. and use that information for targeted marketing when possible. Therefore, with a combination of using a K-Prototype algorithm and applying the Haversine formula, I was able to show the unique clusters of our students and, in the interactive map I created, allow for filtering based on distance from the college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa477865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module for data visualization\n",
    "from plotnine import *\n",
    "import plotnine\n",
    "\n",
    "# Import module for k-prototype cluster\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "# Format scientific notation from Pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, \n",
    "enroll = (pd.read_csv('202110 - 202280 FA, SP, SU HC 20th-D.csv', encoding = 'cp1252')\n",
    "              .rename(columns = str.lower)\n",
    "         )\n",
    "\n",
    "# For practice, isolate a single semester as the K-Prototype Altorithm is time intensive\n",
    "enroll = enroll[enroll['term'] == 202110]\n",
    "\n",
    "# Sort values by term and id\n",
    "enroll.sort_values(['term', 'id'], ascending = [True, True]).reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2396677",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing data\n",
    "miss_data = pd.DataFrame({'# missing':enroll.isna().sum()})\n",
    "\n",
    "miss_data['% rep missing'] = enroll.isna().sum()/len(enroll) * 100\n",
    "\n",
    "miss_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a67ace",
   "metadata": {},
   "source": [
    "### Columns To Eliminate Or Impute\n",
    "\n",
    "  * *pidm*, *lname*, *fname*, *mi*, *rescode*, *degcode*, *majr*, *mrtl*, *ecode*, *race_desc*, *cnty*, *addr#*, *edgoal*, *goaldesc*, *program*, *dob*, *confid*\n",
    "    * Remove all of these because they are either unnecessary for the k-prototype algorithm or repetitive\n",
    "  * *age*: Missing just one value. Therefore, enroll['age'].isna() == False\n",
    "  * *gender*: Missing just five values. Therefore, enroll['gender'].isna() == False\n",
    "  * *cnty_desc1*: Missing 165 values (0.432%). Therefore, enroll['cnty_desc1'].isna() == False\n",
    "  * *ethn_desc*: Missing 1725 values, 4.514%. Therefore, I will create an additional category, in essence, 'missing'.\n",
    "  * *mrtl*: While there may be some benefit in predictive analysis with the marital status of the individual, we are missing nearly 62% of the entries, which is too much to impute given its a categorical variable. It is unclear what value could be derived from the remaining 38%. After completing the first run through, I might add this variable back in to see if the modeling produces any more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45690d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pare down dataframe based on criteria above\n",
    "enroll = enroll[['id', 'term', 'age', 'totcr', 'status', 'stype', 'resd_desc', 'degree',\n",
    "                 'majr_desc1', 'gender', 'ethn_desc', 'cnty_desc1']]\n",
    "\n",
    "# Fill missing values for *ethn_desc* with 'missing', effectively creating\n",
    "# a new category\n",
    "enroll['ethn_desc'] = enroll['ethn_desc'].fillna('missing')\n",
    "\n",
    "# Change the *term* column to a category as it is not actually an \n",
    "# integer\n",
    "enroll['term'] = enroll['term'].astype('str')\n",
    "\n",
    "# Filter out the NaN values as there were not enough to warrant\n",
    "# data imputation\n",
    "mask1 = enroll['age'].isna() == False\n",
    "mask2 = enroll['gender'].isna() == False\n",
    "mask3 = enroll['cnty_desc1'].isna() == False\n",
    "\n",
    "enroll = enroll[mask1 & mask2 & mask3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaa29f",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "After removing the extraneous data and addressing missing values, we are left with 38044 observations and 12 columns. The model will be ran without the *id* column as it will provide no predictive value. The *term* column will be used as a filter to assess whether the clusters of students who enroll vary depending on the semester (i.e. Fall vs. Spring semester). Beyond these two columns, the features are as follows:\n",
    "\n",
    "There are eight categorical variables and two float variables. The float variables will be centered and scaled. The categorical variables will be separated into indicator variables.\n",
    "\n",
    "Let's inspect the nature of the categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6565784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the number of unique values for each category, ignoring\n",
    "# the 'id' and 'term' column\n",
    "enroll.select_dtypes('object').nunique()\n",
    "\n",
    "#for i in list(enroll['ethn_desc'].unique()):\n",
    "#    print(\"  * \" + \"'\" + i + \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f22ca0",
   "metadata": {},
   "source": [
    "The *status* columns has four unique categories:\n",
    "\n",
    "  * 'LH': Students who are enrolled in fewer than 6 credits (< 6)\n",
    "  * 'HT': Students who are enrolled in 6 credits but less than 12 (6 <= x < 12)\n",
    "  * 'FT': Students who are enrolled in *at least* 12 credits. (>= 12)\n",
    "  * 'W': Students who withdrew from the semester\n",
    "  \n",
    "*stype* has six unique categories:\n",
    "\n",
    "  * 'A': Auditors\n",
    "  * 'G': Guest students\n",
    "  * 'H': High school students\n",
    "  * 'N': New Students\n",
    "  * 'T': Transfer students\n",
    "  * 'C': Continuing students\n",
    "  \n",
    "*resd_desc* has 10 categories:\n",
    "\n",
    "  * 'Kansas Resident-out of BC': Student who lives in KS but not in the Co.\n",
    "  * 'Out of state Resident': Student who is not from KS\n",
    "  * 'International': International student\n",
    "  * 'County Resident': Co. resident\n",
    "  * 'Undocumented-out of BC': Undocumented student who is not a Co. resident\n",
    "  * 'Appl for Perm Res/Asylum/TPS': Perm Resident?\n",
    "  * 'Undocumented-BC': Undocumented student from Co.\n",
    "  * 'Appl for Perm Res/Asylum OOS': Perm Resident?\n",
    "  * 'Appl for Perm Res/Asylum': Perm Resident?\n",
    "  * 'App for Per Res/Asylum OOS/TPS': Perm Resident?\n",
    "\n",
    "*degree* has 14 categories:\n",
    "  * 'Associate in Arts'\n",
    "  * 'Associate in Science'\n",
    "  * 'Associate in Applied Science'\n",
    "  * 'Associate in General Studies'\n",
    "  * 'Non Degree Seeking'\n",
    "  * '30 but less than 45 cr hrs'\n",
    "  * 'Certification', \n",
    "  * '45 but less than 60 cr hrs'\n",
    "  * 'Preparatory Coursework'\n",
    "  * '16 but less than 30 cr hrs'\n",
    "  * 'Degree-seeking non-BCCC'\n",
    "  * 'CERTB 30-44 cr hrs'\n",
    "  * 'CERTA less than 30 cr hrs'\n",
    "  * 'CERTC 45-59 cr hrs'\n",
    "  \n",
    "*majr_desc1* has 117 categories:\n",
    "  * 'Psychology'\n",
    "  * 'Pre-Physical Therapy'\n",
    "  * 'Pre-Engineering'\n",
    "  * 'Pre-Computer Science'\n",
    "  * 'Pre-Nursing/Health Science'\n",
    "  * 'Political Science'\n",
    "  * 'Mathematics'\n",
    "  * 'Accounting'\n",
    "  * 'Liberal Arts'\n",
    "  * 'Elementary Education'\n",
    "  * 'Nursing'\n",
    "  * 'Fire Science'\n",
    "  * 'Pre-Veterinarian'\n",
    "  * 'Business Technology'\n",
    "  * 'History'\n",
    "  * 'Sociology/Social Work'\n",
    "  * 'Business Administration'\n",
    "  * 'Undeclared'\n",
    "  * 'Music'\n",
    "  * 'Elementary Education/BEST'\n",
    "  * 'Internetworking Management'\n",
    "  * 'Biological Science'\n",
    "  * 'Engineering Technology'\n",
    "  * 'Pre-Medicine'\n",
    "  * 'English/Literature'\n",
    "  * 'Theatre Performance'\n",
    "  * 'Criminal Justice'\n",
    "  * 'Pre-Physician Assistant'\n",
    "  * 'Software Development'\n",
    "  * 'Music Education'\n",
    "  * 'Agriculture'\n",
    "  * 'Chemistry'\n",
    "  * 'Health Sciences'\n",
    "  * 'Culinary Arts'\n",
    "  * 'Accounting Assistant'\n",
    "  * 'Fine Arts and Communication'\n",
    "  * 'Humanities,Soc/Beh Sciences'\n",
    "  * 'Farm and Ranch Manangement'\n",
    "  * 'Marketing, Mgmt, Entrepreneur'\n",
    "  * 'Secondary Education'\n",
    "  * 'Digital Media'\n",
    "  * 'Cyber Security'\n",
    "  * 'Art'\n",
    "  * 'Athletic Training'\n",
    "  * 'Interactive, Digital & 3D Tech'\n",
    "  * 'Data Analytics'\n",
    "  * 'Mass Communication-Sport Media'\n",
    "  * 'Mass Communication-Radio/TV'\n",
    "  * 'Mass Communication-Journalism'\n",
    "  * 'Pre-Pharmacy'\n",
    "  * 'Sport Management'\n",
    "  * 'Foreign Language'\n",
    "  * 'Web Development'\n",
    "  * 'Physician Coding'\n",
    "  * 'Speech Communication'\n",
    "  * 'Hotel Management'\n",
    "  * 'Physics'\n",
    "  * 'Mass Communications'\n",
    "  * 'Early Childhood Education'\n",
    "  * 'Religion'\n",
    "  * 'Automotive Technology'\n",
    "  * 'Theatre Technical'\n",
    "  * 'Exercise Science'\n",
    "  * 'Economics'\n",
    "  * 'Automotive Collision Repair'\n",
    "  * 'Welding Technology'\n",
    "  * 'Marketing'\n",
    "  * 'Entrepreneurship'\n",
    "  * 'Preparatory Cousework 1 year'\n",
    "  * 'Eng Tech-Drafting'\n",
    "  * 'Education and Public Services'\n",
    "  * 'Agribusiness'\n",
    "  * 'Science, Engineering, and Math'\n",
    "  * 'Database Administration'\n",
    "  * 'Electrical Apprenticeship'\n",
    "  * 'Business Medical Specialist'\n",
    "  * 'Livestock Mgmt/Merchandising'\n",
    "  * 'Prep Coursework Grad 4 years'\n",
    "  * 'Food Science Business'\n",
    "  * 'Homeland Security'\n",
    "  * 'Philosophy'\n",
    "  * 'Business and Industry'\n",
    "  * 'Restaurant Management'\n",
    "  * 'Dance'\n",
    "  * 'Unmanned Aircraft Systems'\n",
    "  * 'Surveying Technology'\n",
    "  * 'Pre Nursing'\n",
    "  * 'Workforce Development'\n",
    "  * 'Eng Tech-Manufacturing'\n",
    "  * 'Food Science Technology'\n",
    "  * 'Engineering Graph Technology'\n",
    "  * 'Transfer Major'\n",
    "  * 'Construction Trades Apprentice'\n",
    "  * 'Nurse Aide'\n",
    "  * 'Plumber/Pipefitter Apprentice'\n",
    "  * 'Construction Technology'\n",
    "  * 'Unified Teaching'\n",
    "  * 'Culinary Arts-Culinarian'\n",
    "  * 'Adv'd Emerg Med Tech'\n",
    "  * 'Diesel Technology'\n",
    "  * 'Culinary Arts-Sous Chef'\n",
    "  * 'Early Childhood CDA'\n",
    "  * 'Emergency Medical Technician'\n",
    "  * 'Eng Tech-Industrial Controls'\n",
    "  * 'Patient Care Pathways'\n",
    "  * 'Preparatory Coursework 2 years'\n",
    "  * 'Food Science and Safety'\n",
    "  * 'Medication Aide'\n",
    "  * 'MOS Test Prep'\n",
    "  * 'Professional Culinary Arts'\n",
    "  * 'Pre-Health Professions'\n",
    "  * 'Oper Train'g Assist'd Living'\n",
    "  * 'Early Ed Child Unified Edu'\n",
    "  * 'Bus Admin 75 WSU'\n",
    "  * 'Prof Culinary Arts-Sous Chef'\n",
    "  * 'Prof Culinary Arts-Culinarian'\n",
    "  * 'Fire Science Leadership'\n",
    "  \n",
    "*gender* has three categories:\n",
    "  * 'M': Male\n",
    "  * 'F': Female\n",
    "  * 'N': Neutral?\n",
    " \n",
    "*ethn_desc* has 9 categories:\n",
    "  * 'Black'\n",
    "  * 'missing'\n",
    "  * 'Caucasian/White'\n",
    "  * 'Hispanic'\n",
    "  * 'Undeclared'\n",
    "  * 'Mixed'\n",
    "  * 'Asian'\n",
    "  * 'American Indian/Alaskan'\n",
    "  * 'Pacific Islander/Hawaiian'\n",
    "  \n",
    "*cnty_desc1* has 421:\n",
    "  * Due to the space this would take up, it will not be inlcuded here. Also it is not particularly helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3324e0",
   "metadata": {},
   "source": [
    "### Examine the numerical data\n",
    "\n",
    ".desribe() naturally isolates the numerical data stored in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca257b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b775ecd",
   "metadata": {},
   "source": [
    "The average number of credit hours taken over all of these semesters is 8.518 with a standard deviation of 4.633. The average age is 23.692 with a standard devation of 8.245. The minimum credit hours enrolled in is 0.000 whil the minimum age of a student enrolled in the semesters examined here is 13.583. The maximum age enrolled and credit hours enrolled is 87.250 and 29, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ca456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of the stypes. It will include a percent rep column,\n",
    "# and the total number of students in that student type (headcount)\n",
    "df_stype = (pd.DataFrame(enroll['stype'].value_counts())\n",
    "              .reset_index()\n",
    "           )\n",
    "df_stype['Percentage'] = df_stype['stype']/enroll['stype'].value_counts().sum()\n",
    "df_stype.rename(columns = {'index':'Stype', 'stype':'Total'}, inplace = True)\n",
    "df_stype = df_stype.sort_values('Total', ascending = True).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22044875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe still focuses on stype but totals the \n",
    "# headcount (total), gives total credit hours per \n",
    "# stype (totcr), and give the average age (age). \n",
    "\n",
    "df_stype2 = (enroll.groupby('stype')\n",
    "                   .agg({\n",
    "                         'stype':'count',\n",
    "                         'totcr':'sum',\n",
    "                         'age':'mean'\n",
    "                        }\n",
    "                 ).rename(columns = {'stype':'total'}\n",
    "                 ).reset_index()\n",
    "                  .sort_values('total', ascending = True)\n",
    "            )\n",
    "\n",
    "df_stype2\n",
    "\n",
    "output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data. Interesting thing I learned here. ggplot from R has been \n",
    "# imported into Python, which is awesome because I know it much \n",
    "# better than matplotlib.\n",
    "\n",
    "plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(data = df_stype2) + \n",
    "      geom_bar(aes(x = 'stype',\n",
    "                   y = 'totcr'),\n",
    "               fill = np.where(df_stype2['stype'] == 'N', '#981220', '#80797c'),\n",
    "               stat = 'identity') + \n",
    "      labs(title = 'Stype that has the highest crhr',\n",
    "           x = 'Stype',\n",
    "           y = 'Frequency') +\n",
    "      scale_x_discrete(limits = df_stype2['stype'].tolist()) + \n",
    "      theme_minimal() +\n",
    "      coord_flip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f3438",
   "metadata": {},
   "source": [
    "**Standardizing the numerical data**\n",
    "\n",
    "K-means is sensitive to the scale of the numerical data. Therefore, we will need to standardize *age* and *totcr* because one is on a scale from 0-29 and the other is on a scale from 14-87. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to create a special enrollment dataframe (enroll_sp) \n",
    "# that has just the numeric values\n",
    "enroll_sp = enroll[['id', 'term', 'age', 'totcr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize standardscaler\n",
    "std_scaler = StandardScaler()\n",
    " \n",
    "# Standardize the numeric data--*age* and *totcr*\n",
    "df_scaled = std_scaler.fit_transform(enroll_sp[['age', 'totcr']].to_numpy())\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=['age', 'totcr'])\n",
    "\n",
    "# Import the ids back into the dataframe for merging\n",
    "df_scaled['id'] = enroll_sp['id']\n",
    " \n",
    "# Merge the filtered dataframe's categorical variables with the \n",
    "# standardized numeric variables.\n",
    "filt_df = df_scaled.merge(enroll, how = 'left', on = 'id')\n",
    "\n",
    "# Reorganize the variables and drop the unstandardized age and totcr \n",
    "# columns.\n",
    "filt_df = (filt_df[['id', 'term', 'age_x', 'totcr_x', 'status', 'stype', 'resd_desc', \n",
    "                   'degree', 'majr_desc1', 'gender', 'ethn_desc', 'cnty_desc1']]\n",
    "                .rename(\n",
    "                   columns = {\n",
    "                       'age_x':'age',\n",
    "                       'totcr_x':'totcr'\n",
    "                   })\n",
    "          )\n",
    "\n",
    "# Finally, drop the duplicate rows that are generated by this merger. I have \n",
    "# not figured out why it is duplicating rows (2.28.23)\n",
    "# These duplications also generate some NaN values that were not there previously.\n",
    "# Literally just one row of NaN values.\n",
    "filt_df = (filt_df.drop_duplicates(subset = ['id', 'term'])\n",
    "               .reset_index(drop = True)\n",
    "               .drop(['id', 'term'], axis = 1)\n",
    "               .dropna()\n",
    "          )\n",
    "\n",
    "# Finally, for the K-Prototype Algorithm, you have to store the index of the categorical\n",
    "# columns. That is what we are doing below.\n",
    "cat_columns_pos = [filt_df.columns.get_loc(i) for i in list(filt_df.select_dtypes('object').columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9632fb",
   "metadata": {},
   "source": [
    "The K-Prototype clustering algorithm runs best on a matrix, so we want to convert the dataframe (excluding the *id* and *term* column) into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03819fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to matrix\n",
    "dfmatrix = filt_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa787c0d",
   "metadata": {},
   "source": [
    "### Determine Appropriate Number Of Clusters\n",
    "Now, as is customary to discover the appropriate number of clusters, we will use the *elbow method* to ascertain best number of clusters. Usually, with all numerical data as in the case of K-Means, we would use the within sum of squares errors (WSSE) with Euclidian distance, however, we cannot use Euclidean distance for categorical variables. Therefore, for K-Prototype, we will use the cost function that combines the calculation for numerical and categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal K using the elbow method\n",
    "cost = []\n",
    "\n",
    "for cluster in range(1, 10):\n",
    "    try:\n",
    "        kprototype = KPrototypes(n_jobs = -1, n_clusters = cluster, init = 'Huang', random_state = 0)\n",
    "        kprototype.fit_predict(dfmatrix, categorical = cat_columns_pos)\n",
    "        cost.append(kprototype.cost_)\n",
    "        #print('Cluster Initiation: {}'.format(i))\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "# Converting the results into a dataframe and plotting them\n",
    "df_cost = pd.DataFrame({'Cluster':range(1, 10), 'Cost':cost})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aeb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the % change with each successive K group that is added.\n",
    "perc_diff = {}\n",
    "\n",
    "for i in range(1, 9, 1):\n",
    "    perc_diff[i+1] = (((df_cost['Cost'][i] - df_cost['Cost'][i-1])/df_cost['Cost'][i-1])*100)\n",
    "    \n",
    "\n",
    "(df_cost.merge(pd.DataFrame.from_dict(perc_diff, orient = 'index')\n",
    "        .reset_index()\n",
    "        .rename(columns = {'index':'Cluster',\n",
    "                           0:'% Change'}), how = 'left', on = 'Cluster')\n",
    "        .fillna(\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de48d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scree Plot of the Cost function to choose optimal K value\n",
    "plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(data = df_cost)+\n",
    "    geom_line(aes(x = 'Cluster',\n",
    "                  y = 'Cost'))+\n",
    "    geom_point(aes(x = 'Cluster',\n",
    "                   y = 'Cost'))+\n",
    "    geom_label(aes(x = 'Cluster',\n",
    "                   y = 'Cost',\n",
    "                   label = 'Cluster'),\n",
    "               size = 10,\n",
    "               nudge_y = 1000) +\n",
    "    labs(title = 'Optimal number of cluster with Elbow Method')+\n",
    "    xlab('Number of Clusters k')+\n",
    "    ylab('Cost')+\n",
    "    theme_minimal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bb7e0",
   "metadata": {},
   "source": [
    "Based on the *scree plot* above, we will start with K = 3 as it appears to be the optimal number of clusters. I found this number of clusters to be insufficient for driving insight. Therefore, I played with different number of clusters and found seven to be the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the clusters\n",
    "kprototype = KPrototypes(n_jobs = -1, n_clusters = 7, init = 'Huang', random_state = 0)\n",
    "kprototype.fit_predict(dfmatrix, categorical = cat_columns_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2fda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster centroids\n",
    "kprototype.cluster_centroids_\n",
    "\n",
    "# check the iteration of the clusters created\n",
    "kprototype.n_iter_\n",
    "\n",
    "# Check the cost of the clusters created\n",
    "kprototype.cost_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540ed8",
   "metadata": {},
   "source": [
    "### Interpreting The Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb831dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the clusters to the dataframe\n",
    "filt_df['id'], filt_df['term'] = enroll_sp['id'], enroll_sp['term']\n",
    "\n",
    "filt_df['cluster labels'] = kprototype.labels_\n",
    "filt_df['segment'] = filt_df['cluster labels'].map({0:'First', 1:'Second', 2:'Third'})\n",
    "\n",
    "# Order clusters\n",
    "filt_df['segment'] = filt_df['segment'].astype('category')\n",
    "filt_df['segment'] = filt_df['segment'].cat.reorder_categories(['First', 'Second', 'Third'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec57b4c",
   "metadata": {},
   "source": [
    "Now we can dig into each cluster to see what characteristics are common between all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64224f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a dataframe with the original ages and total credit hours enrolled\n",
    "# Essentially destandardizing them.\n",
    "age_cr = enroll[['id', 'age', 'totcr']].reset_index(drop = True)\n",
    "\n",
    "# Merge the filtered dataframe with the original ages and credit hours\n",
    "integrated_clusters = (filt_df.merge(age_cr, how = 'left', on = 'id').drop(['age_x', 'totcr_x'], axis = 1)\n",
    "                           [['id', 'term', 'age_y', 'totcr_y', 'status', 'stype', 'resd_desc', \n",
    "                             'degree', 'majr_desc1', 'gender', 'ethn_desc', 'cnty_desc1', \n",
    "                             'cluster labels', 'segment']]\n",
    "                             .rename(columns = {'age_y':'age',\n",
    "                                                'totcr_y':'totcr'})\n",
    "                      )\n",
    "\n",
    "integrated_clusters1 = integrated_clusters[integrated_clusters['segment'] == 'First']\n",
    "integrated_clusters2 = integrated_clusters[integrated_clusters['segment'] == 'Second']\n",
    "integrated_clusters3 = integrated_clusters[integrated_clusters['segment'] == 'Third']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mode = {}\n",
    "for i in list(integrated_clusters1.select_dtypes('object').columns):\n",
    "    cat_mode[i] = integrated_clusters1[i].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d119fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mean = {}\n",
    "\n",
    "for i in list(integrated_clusters1.select_dtypes('float64').columns):\n",
    "    num_mean[i] = integrated_clusters1[i].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f00dfe",
   "metadata": {},
   "source": [
    "A quick perusal shows that just looking at the means and modes of these students is not going to be enough to glean anything helpful from each grouping. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780f944",
   "metadata": {},
   "source": [
    "# Longitude, Latitude Converter\n",
    "\n",
    "In this script, we use the requests library to send a GET request to the OpenStreetMap Nominatim API. We pass the address as a query parameter and limit the results to one match. We also specify the response format as JSON.\n",
    "\n",
    "Once we receive the JSON response, we extract the latitude and longitude from the first match, if one exists. If there are no matches, we leave the latitude and longitude fields empty.\n",
    "\n",
    "Finally, we write the address, latitude, and longitude to the output CSV file.\n",
    "\n",
    "Note that OpenStreetMap Nominatim has usage limits, so you should read the usage policy and make sure that your use of the service complies with the terms of use.\n",
    "\n",
    "This process using openstreetmap did work for converting the addresses to longitude and latitude. It has in its user agreement not to do big batch dataprocessing. I'm not sure if one semester is big batch. In this case, it was about 6100 addresses. It only allows one request a second. Therefore, it took one hour and fourty-two minutes to download, but to my surprise, it worked! You'll notice, *nominatim* is the extension of the library *geopy* that is used in the call geopy.geocoders.Nominatim(user_agent = 'MyCoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "# OpenStreetMap Nominatim API URL\n",
    "NOMINATIM_URL = 'https://nominatim.openstreetmap.org/search'\n",
    "\n",
    "# CSV file containing addresses\n",
    "input_file = '202310 Addresses Missing.csv'\n",
    "\n",
    "# CSV file to write output\n",
    "output_file = 'long_lat_202310_missing.csv'\n",
    "\n",
    "# Open input file and output file\n",
    "with open(input_file, 'r') as csv_input, open(output_file, 'w', newline='') as csv_output:\n",
    "    # Create CSV reader and writer objects\n",
    "    reader = csv.reader(csv_input)\n",
    "    writer = csv.writer(csv_output)\n",
    "\n",
    "    # Write header row to output file\n",
    "    writer.writerow(['Address', 'Latitude', 'Longitude'])\n",
    "\n",
    "    # Loop through each row in the input file\n",
    "    for row in reader:\n",
    "        # Get the address from the current row\n",
    "        address = row[0]\n",
    "\n",
    "        # Send a GET request to the OpenStreetMap Nominatim API\n",
    "        response = requests.get(NOMINATIM_URL, params={'q': address, 'format': 'json', 'limit': 1})\n",
    "\n",
    "        # Parse the JSON response\n",
    "        json_data = response.json()\n",
    "\n",
    "        # Get the latitude and longitude from the JSON response\n",
    "        if len(json_data) > 0:\n",
    "            latitude = json_data[0]['lat']\n",
    "            longitude = json_data[0]['lon']\n",
    "        else:\n",
    "            latitude = ''\n",
    "            longitude = ''\n",
    "\n",
    "        # Write the address, latitude, and longitude to the output file\n",
    "        writer.writerow([address, latitude, longitude])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fecab",
   "metadata": {},
   "source": [
    "### The Code Below Is For Accessing Mapquest Geocode API\n",
    "\n",
    "Mapquest did an outstanding job. You're allowed 15,000 requests a month. So that's like two semester's worth, but it did find every single address and came back with the correct longitude and latitude. It is limited, mostly, to the United States. The format for the address file is, no headers, just a matrix, with the first column as the street address, the second column is the city, the third column is the state, the fourth is the zip code. It does generally only load one row per second so 6100 takes an hour and 20 to an hour and 40 minutes. I did one load at night and it ran it a little faster. During the day, expect a semester of 6500 students to take two to three hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b614a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "# MapQuest Geocoding API URL\n",
    "MAPQUEST_URL = 'https://www.mapquestapi.com/geocoding/v1/address'\n",
    "\n",
    "# API key (replace with your own)\n",
    "API_KEY = 'k7vgChZQLe0QFnfkRp6RjXGOW5Krg84z'\n",
    "\n",
    "# CSV file containing addresses\n",
    "input_file = '202310 Addresses.csv'\n",
    "\n",
    "# CSV file to write output\n",
    "output_file = 'long_lat_202310.csv'\n",
    "\n",
    "# Open input file and output file\n",
    "with open(input_file, 'r') as csv_input, open(output_file, 'w', newline='') as csv_output:\n",
    "    # Create CSV reader and writer objects\n",
    "    reader = csv.reader(csv_input)\n",
    "    writer = csv.writer(csv_output)\n",
    "\n",
    "    # Write header row to output file\n",
    "    writer.writerow(['address', 'longitude', 'latitude'])\n",
    "\n",
    "    # Loop through each row in the input file\n",
    "    for row in reader:\n",
    "        # Get the address components from the current row\n",
    "        street, city, state, zip_code = row\n",
    "\n",
    "        # Build the address string\n",
    "        address = f'{street}, {city}, {state} {zip_code}'\n",
    "\n",
    "        # Send a GET request to the MapQuest Geocoding API\n",
    "        response = requests.get(MAPQUEST_URL, params={'key': API_KEY, 'location': address})\n",
    "\n",
    "        # Parse the JSON response\n",
    "        json_data = response.json()\n",
    "\n",
    "        # Get the latitude and longitude from the JSON response\n",
    "        if json_data['results']:\n",
    "            latitude = json_data['results'][0]['locations'][0]['latLng']['lat']\n",
    "            longitude = json_data['results'][0]['locations'][0]['latLng']['lng']\n",
    "        else:\n",
    "            latitude = ''\n",
    "            longitude = ''\n",
    "\n",
    "        # Write the address, latitude, and longitude to the output file\n",
    "        writer.writerow([address, longitude, latitude])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5af621",
   "metadata": {},
   "source": [
    "# Plot Longitude And Latitude\n",
    "\n",
    "This code comes from the website below:\n",
    "\n",
    "https://towardsdatascience.com/clustering-geospatial-data-f0584f0b04ec\n",
    "\n",
    "The code below works splendedly. To make the geospatial clustering useful for our marketing department, I limited the scope to the metro area, which is where the majority of our students come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db274095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install folium\n",
    "#!pip install geopy\n",
    "#!pip install scikit-learn\n",
    "#!pip install MiniSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for geospatial\n",
    "import folium\n",
    "import geopy\n",
    "\n",
    "# for machine learning\n",
    "from sklearn import preprocessing, cluster\n",
    "import scipy\n",
    "\n",
    "# for deep learning\n",
    "import minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataframe\n",
    "long_lat = pd.read_csv('long_lat_202310.csv')\n",
    "\n",
    "long_lat.columns = [i.title() for i in long_lat.columns]\n",
    "\n",
    "# Filter dataframe to Wichita and the surrounding area. The cities kept \n",
    "# in the filtered data account for ##.##% of all students enrolled for \n",
    "# Spring 2023.\n",
    "\n",
    "long_lat = (long_lat[long_lat['City'].isin(['Wichita', 'El Dorado', 'Andover', 'Derby', 'Augusta', ',Rose Hill',\n",
    "                                            'Haysville', 'Valley Center', 'Bel Aire', 'Towanda', 'Mulvane', 'Benton',\n",
    "                                            'Douglass', 'Park City', 'Maize', 'Goddard', 'Kechi'\n",
    "                                           ])]\n",
    "                                     .reset_index(drop = True)\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curtail dataframe to just the Address, Latitude, and Longitude. \n",
    "# The new 'count' column is just to create a categorie for the \n",
    "# K-Means geospatial algorithm.\n",
    "long_lat = (pd.DataFrame(long_lat.groupby(['Address', 'Latitude', 'Longitude'])['Address'].count())\n",
    "              .rename(columns = {'Address':'Count'})\n",
    "              .reset_index()\n",
    "           )\n",
    "\n",
    "category = []\n",
    "\n",
    "for i in long_lat['Count']:\n",
    "    if i <= 2:\n",
    "        categorie.append('Low')\n",
    "    elif i >= 3 and i < 7:\n",
    "        categorie.append('Med')\n",
    "    else:\n",
    "        categorie.append('High')\n",
    "        \n",
    "long_lat['Categories'] = categorie\n",
    "\n",
    "#pd.DataFrame(long_lat.groupby('Categories')['Address'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f45ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates for the city\n",
    "city = 'Wichita'\n",
    "\n",
    "# Get location\n",
    "locator = geopy.geocoders.Nominatim(user_agent = 'MyCoder')\n",
    "location = locator.geocode(city)\n",
    "print(location)\n",
    "\n",
    "# Keep latitude and longitude only\n",
    "location = [location.latitude, location.longitude]\n",
    "print('[lat, long]:', location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57aaac7",
   "metadata": {},
   "source": [
    "Now *folium* will be used to create a map. It is a convenient package that allows us to plot interactive maps without needing to load a shapefile. Each store shall be identified by a point with size proportional to its current staff and color base on its cost. I'm also going to add a small piece of HTML code to the default map to display the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8aa57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 'Latitude', 'Longitude'\n",
    "color = 'Categories'\n",
    "size = 'Count'\n",
    "popup = 'Address'\n",
    "data = long_lat.copy()\n",
    "\n",
    "# create color column\n",
    "lst_colors = ['red', 'green', 'orange']\n",
    "lst_elements = sorted(list(long_lat[color].unique()))\n",
    "data['color'] = data[color].apply(lambda x: lst_colors[lst_elements.index(x)])\n",
    "\n",
    "\n",
    "# create size column (scaled)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (300, 1000))\n",
    "data['size'] = scaler.fit_transform(data[size].values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Initialize the map with the starting location\n",
    "map_ = folium.Map(location = location, tiles = 'cartodbpositron', zoom_start = 11)\n",
    "\n",
    "# add points\n",
    "data.apply(lambda row: folium.CircleMarker(location = [row[x], row[y]], popup = row[popup],\n",
    "                                           color = row['color'], fill = True,\n",
    "                                           size = row['size']).add_to(map_), axis = 1)\n",
    "\n",
    "# add html legend\n",
    "legend_html = \"\"\"<div style = \"position:fixed; bottom:10px; left:10px; border:2px solid black; \n",
    "                z-index:9999; font-size:14px;\">&nbsp;<b>\"\"\"+color+\"\"\":</b>\n",
    "                <br>\"\"\"\n",
    "\n",
    "for i in lst_elements:\n",
    "    legend_html = legend_html+\"\"\"&nbsp;<i class=\"fa fa-circle \n",
    "                              fa-1x\" style = \"color:\"\"\"+lst_colors[lst_elements.index(i)]+\"\"\"\">\n",
    "                              </i>&nbsp;\"\"\"+str(i)+\"\"\"<br>\"\"\"\n",
    "\n",
    "legend_html = legend_html+\"\"\"</div>\"\"\"\n",
    "\n",
    "map_.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# plot the map\n",
    "map_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966bc290",
   "metadata": {},
   "source": [
    "### Create Scree Plot To Find Best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = long_lat[[\"Latitude\",\"Longitude\"]]\n",
    "max_k = 10\n",
    "\n",
    "## iterations\n",
    "distortions = [] \n",
    "for i in range(1, max_k+1):\n",
    "    if len(X) >= i:\n",
    "        model = cluster.KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        model.fit(X)\n",
    "        distortions.append(model.inertia_)\n",
    "        \n",
    "## best k: the lowest derivative\n",
    "k = [i*100 for i in np.diff(distortions,2)].index(min([i*100 for i \n",
    "     in np.diff(distortions,2)]))\n",
    "\n",
    "## plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(1, len(distortions)+1), distortions)\n",
    "ax.axvline(k, ls='--', color=\"red\", label=\"k = \"+str(k))\n",
    "ax.set(title='The Elbow Method', xlabel='Number of clusters', \n",
    "       ylabel=\"Distortion\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f999897",
   "metadata": {},
   "source": [
    "## Create the K-Means Geospatial Algorithm And Apply It To The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e589539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of clusters *k* at 5\n",
    "k = 15\n",
    "\n",
    "# Initialize k-means model\n",
    "model = cluster.KMeans(n_clusters=k, init='k-means++')\n",
    "\n",
    "# clustering\n",
    "X = long_lat[[\"Latitude\",\"Longitude\"]]\n",
    "\n",
    "dtf_X = X.copy()\n",
    "\n",
    "# find real centroids\n",
    "dtf_X[\"cluster\"] = model.fit_predict(X)\n",
    "\n",
    "closest, distances = scipy.cluster.vq.vq(model.cluster_centers_, \n",
    "                     dtf_X.drop(\"cluster\", axis=1).values)\n",
    "\n",
    "dtf_X[\"centroids\"] = 0\n",
    "for i in closest:\n",
    "    dtf_X[\"centroids\"].iloc[i] = 1  \n",
    "\n",
    "# add clustering info to the original dataset\n",
    "long_lat[[\"cluster\",\"centroids\"]] = dtf_X[[\"cluster\",\"centroids\"]]\n",
    "\n",
    "long_lat.sample(5)\n",
    "\n",
    "# View number of addresses in each cluster grouping\n",
    "(pd.DataFrame(long_lat.groupby('cluster')['Address'].count())\n",
    "   .rename(columns = {'Address':'Count'})\n",
    "   .reset_index()\n",
    "   .sort_values('Count', ascending = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3895906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x=\"Latitude\", y=\"Longitude\", data=long_lat, \n",
    "                palette=sns.color_palette(\"bright\",k),\n",
    "                hue='cluster', size=\"centroids\", size_order=[1,0],\n",
    "                legend=\"brief\", ax=ax).set_title('Clustering(k=' + str(k) + ')')\n",
    "\n",
    "th_centroids = model.cluster_centers_\n",
    "ax.scatter(th_centroids[:,0], th_centroids[:,1], s=50, c='black', \n",
    "           marker=\"x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17404d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cluster.AffinityPropagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce8e90",
   "metadata": {},
   "source": [
    "### Plot the K-Means Clusters On The Geospatial Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objects for ploting\n",
    "x, y = 'Latitude', 'Longitude'\n",
    "color = 'cluster'\n",
    "size = 'Count'\n",
    "popup = 'Address'\n",
    "marker = 'centroids'\n",
    "data = long_lat.copy()\n",
    "\n",
    "# create color column\n",
    "lst_colors = ['#%06X' % np.random.randint(0, 0xFFFFFF) for i in range(len(lst_elements))]\n",
    "lst_elements = sorted(list(long_lat[color].unique()))\n",
    "data['color'] = data[color].apply(lambda x: lst_colors[lst_elements.index(x)])\n",
    "\n",
    "# create size column (scaled)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (300, 1000))\n",
    "data['size'] = scaler.fit_transform(data[size].values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Initialize the map with the starting location\n",
    "map_ = folium.Map(location = location, tiles = 'cartodbpositron', zoom_start = 11)\n",
    "\n",
    "# add points\n",
    "data.apply(lambda row: folium.CircleMarker(location = [row[x], row[y]], popup = row[popup],\n",
    "                                           color = row['color'], fill = True,\n",
    "                                           size = row['size']).add_to(map_), axis = 1)\n",
    "\n",
    "# add html legend\n",
    "legend_html = \"\"\"<div style = \"position:fixed; bottom:10px; left:10px; border:2px solid black; \n",
    "                z-index:9999; font-size:14px;\">&nbsp;<b>\"\"\"+color+\"\"\":</b>\n",
    "                <br>\"\"\"\n",
    "\n",
    "for i in lst_elements:\n",
    "    legend_html = legend_html+\"\"\"&nbsp;<i class=\"fa fa-circle \n",
    "                              fa-1x\" style = \"color:\"\"\"+lst_colors[lst_elements.index(i)]+\"\"\"\">\n",
    "                              </i>&nbsp;\"\"\"+str(i)+\"\"\"<br>\"\"\"\n",
    "\n",
    "legend_html = legend_html+\"\"\"</div>\"\"\"\n",
    "\n",
    "lst_elements = sorted(list(long_lat[marker].unique()))\n",
    "data[data[marker]==1].apply(lambda row: folium.Marker(location = [row[x], row[y]], popup = row[marker], draggable = False, icon = folium.Icon(color = 'black')).add_to(map_), axis =1)\n",
    "map_.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# plot the map\n",
    "\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4437f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat.to_excel(r'C:\\pathway_to_files\\Enrollments\\K-Means_202310.xlsx', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21535715",
   "metadata": {},
   "source": [
    "### Calculate The Distance From Metro Campus For All of Our Students\n",
    "\n",
    "We use the Haversine formula to calculate the distance from one of our two major Campuses (Andover or El Dorado). If the distance from one of the addresses is closer to Andover than El Dorado, then we store that distance, otherwise we store the distance from El Dorado to the student address. This way, we are storing the shortest distance from the student's home to a major Campus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Fixed point coordinates\n",
    "andover_lat = 37.7062631  # Andover BCC Latitude\n",
    "andover_lon = -97.1276288 # Andover BCC longitude\n",
    "eldo_lat = 37.8073112 # Eldo BCC Latitude\n",
    "eldo_lon = -96.8851877 # Eldo BCC Longitude\n",
    "\n",
    "# Haversine formula to calculate distance between two points\n",
    "def calculate_distance(fix_lat1, fix_lon1, dflat, dflon, fix_lat2, fix_lon2):\n",
    "    # Convert latitude and longitude to radians\n",
    "    rlat1, rlon1, rlat2, rlon2, rlat3, rlon3 = map(radians, [fix_lat1, fix_lon1, dflat, dflon, fix_lat2, fix_lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlon = rlon2 - rlon1\n",
    "    dlat = rlat2 - rlat1\n",
    "    a = sin(dlat / 2)**2 + cos(rlat1) * cos(rlat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance1 = 3958.8 * c # Radius of Earth is 3958.8\n",
    "    \n",
    "    dlon2 = rlon2 - rlon3\n",
    "    dlat2 = rlat2 - rlat3\n",
    "    a2 = sin(dlat2 / 2)**2 + cos(rlat3) * cos(rlat2) * sin(dlon2 / 2)**2\n",
    "    c2 = 2 * atan2(sqrt(a2), sqrt(1 - a2))\n",
    "    distance2 = 3958.8 * c2\n",
    "    \n",
    "    if distance1 < distance2:\n",
    "        distance = distance1\n",
    "    else:\n",
    "        distance = distance2\n",
    "        \n",
    "    return distance\n",
    "\n",
    "# Read csv file into a pandas DataFrame\n",
    "df = pd.read_csv('K-Means_202310.csv')\n",
    "\n",
    "# Calculate distance from fixed point for each row\n",
    "df['Distance'] = np.vectorize(calculate_distance)(andover_lat, andover_lon, df['Latitude'], df['Longitude'], eldo_lat, eldo_lon)\n",
    "\n",
    "# Print DataFrame with Distance to folder\n",
    "df.to_excel(r'C:\\pathway_to_file\\Enrollments\\K-Means_202310_distance.xlsx', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
