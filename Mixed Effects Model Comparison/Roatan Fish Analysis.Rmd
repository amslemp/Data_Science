---
title: "Practice"
output: html_document
date: '2023-06-06'
editor_options: 
  markdown: 
    wrap: sentence
---

```{r warning = FALSE, message = FALSE}

library(tidyverse)
library(caret)
library(car)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(randomForest)
library(gbm)
library(mgcv)
library(nnet)


options(scipen = 999)

```

There are several different options for you if you are trying to examine the number
of fish that are in each dive location. We did not have much opportunity to discuss
the goal of your research, which leaves me to infer some of hte details. Given that
you referenced a mixed effects regression (cf. Mixed Effects Model), I assume you are
conducting some form of regression analysis. A Mixed Effects Regression shares several
assumptions with the Multiple Lenear Regression model, though they are not identical.
You were correct to think of a mixed effects model as a good starting place because
your study examines multiplemeasurements over time.

You have expressed concern over whether to average counts across sites or to provide
individual counts. I recommend using individual counts for each site and species,
because it guards against Simpson's Paradox, where averages tend to mislead by
inverting outcomes. Moreover, Central tendency measures, like the mean, often omit
crucial undersandings given by the distribution and variability of the data. In my
seven years as a data scientist, I have consistently found that detailed, granular
data enhances regression analyses and statistical models, avoiding the simplifications
of averages in mixed effects models.Emphasizing data granularity will significantly
benefit your statistical analysis. 

<u>MLR</u> has the following assumptions:

**Linearity**: The relationship between the independent and dependent variables is
linear.
**Independence**: The observations are independent of one another.
**Homoscedasticity**: The variance of the errors is constant across all levels of the
independent variables.
**Normality**: The errors are normally distributed.

Whereas <u>MEM</u> assumptions are:

**Linearity**: The relationship between the independent and dependent variables is
lineear.
**Independence**: The *errors* are independent.
In this case, we do not assume that the *observations* are independent, but only that
the *errors* are independent after accounting for the fixed and random effects. This
is why MEM are suitable for clustered or longitudinal data where observations within
a group or time period might be correlated.
**Homoscedasticity**: The variance of the errors is constant across all levels of the
independent variables.
However, the *total variance* of the observations can vary across groups due to the
random effects.
**Normality**: The errors are normally distributed; this includes the random effects.
**Random Effects**: The random effects are assumed to be normally distributed with a
mean of zero, and they are uncorrelated with the errors.

If the <u>linearity assumption</u> is not met, the estimated coefficients could be
biased, which means that they would not reflect the true relationship between the
predictors and the response variable.
Therefore, predictions based on the model could be inaccurate, especially for values
of the predictors outside the range of the data used to fit the model.

If the <u>independence of errors</u> is violated, the standard errors of the fixed
effects could be underestimated, which would lead to overly optimistic (i.e. too
small) p-values and inflated Type 1 error rates (i.e. falsely rejecting the null
hypothesis). Moreover, the model might not adequately account for the structure of
the data, leading to inefficient estimates.

If the <u>homoscedasticity</u> assumption is violated, the standard errors of the
fixed effects could be incorrect, leading to incorrect p-values and confidence
intervals. Additionally, groups with higher variability might be given too much
weight, which will bias the estimates.

If the <u>normality of error and random effects</u> assumptions are violated, once
again the standard errors of the fixed effects could be incorrect, leading to
incorrect p-values and confidence intervals. Since the model uses the method of
maximum likelihood or restricted maximum likelihood to estimate the parameters, it
may not provide the best possible estimates of the fixed and random effects when this
assumption is violated.

## How Will The Model Performances Be Compared?

The models will be compared using the Mean Squared Error (MSE), which will be
calculated using the following method.

$$ MSE = \frac{1}{n}\sum_{i = 1}^n (y_i - \hat{y}_i)^2$$

where $n$ is the number of observations, $y_i$ is the actual value of the response variable, and $\hat{y}_i$ is the predicted value of the response variable.

## Create Toy Dataset

Since I do not have access to a real dataset,created one from scratch that
might roughly follow what your data shows. Mine will naturally lack the spontaneity
and randomness of your data. Nevertheless, I created the dataset based on some
assumptions and experiences I have with fish at different depths and temperatures
within coral reefs. My underlying assumptions for the dataset:

  * There are more fish, and also smaller fish, at shallower depths. The inverse is
  also true. There are fewer fish, though larger, at deeper depths. 
    * The depths I considered where only from 0 - 65 feet. I created normal 
    distributions (perturbations) around each depth of 15, 30, 45, and 60 feet. 
  * I actually looked up the average water temperatures for each month of the year in
  Roatan, both the max and min, and then incorporated those into the data.
  * I included six different species of fish I found in another dataset
  * The years were three separate years, in this case, 2018, 2021, 2023. 
  * Site is a variable with four separate sites

```{r}

rm(list = ls())

# Set seed for reproducibility
set.seed(101)

# List of species
species <- c('Queen Angelfish', 'Blue Tang', 'Yellowtail Snapper', 'Hogfish', 'Nassau
             Grouper', 'Parrotfish')

# Create a toy dataset
toy_data <- expand.grid(
  
  year = c(2018, 2021, 2023),
  site = c('Site1', 'Site2', 'Site3', 'Site4'),
  week = 1:52,
  species = species
  
)


# Add Depth with perturbation around a normal distribution of depths within specified
# ranges
depths <- c(rnorm(nrow(toy_data)/4, mean=15, sd=2.5), 
            rnorm(nrow(toy_data)/4, mean=30, sd=2.5), 
            rnorm(nrow(toy_data)/4, mean=45, sd=2.5), 
            rnorm(nrow(toy_data)/4, mean=60, sd=2.5))

# Clip depths to specified ranges to maintain realism
depths[depths < 10] <- 10
depths[depths > 20 & depths < 25] <- 25
depths[depths > 35 & depths < 40] <- 40
depths[depths > 50 & depths < 55] <- 55
depths[depths > 65] <- 65

# Assign generated depths to the dataset
toy_data$depth <- depths

# Add other variables
toy_data$temperature = round(runif(nrow(toy_data), min = 79, max = 86), 1)
toy_data$count = rpois(nrow(toy_data), lambda = 20)  # Assuming count follows a Poisson distribution

# Define the temperature ranges for each month
temp_ranges <- data.frame(
  month = c(1:12),
  min_temp = rep(c(78.6, 78.6, 79.2, 80.4, 81.5, 83.3, 83, 83.9, 84.7, 82.8, 80.7, 80.4)),
  max_temp = rep(c(80.4, 80.1, 81, 81.8, 83.7, 84.8, 84.6, 85.5, 86, 85.2, 83.3, 81.8))
)

# Determine the month for each week
toy_data$month <- ceiling(toy_data$week / 4.345)

# Define a function to adjust temperature based on depth
adjust_temperature <- function(min_temp, max_temp, depth) {
  # Calculate adjusted temperature
  adjusted_temp <- min_temp + ((max_temp - min_temp) * (65 - depth) / 55)
  
  # Clip adjusted temperature to specified range to maintain realism
  adjusted_temp[adjusted_temp < min_temp] <- min_temp
  adjusted_temp[adjusted_temp > max_temp] <- max_temp

  return(adjusted_temp)
}

# Assign the appropriate temperature for each month
toy_data <- toy_data %>%
  left_join(temp_ranges, by = "month") %>%
  mutate(temperature = mapply(adjust_temperature, min_temp, max_temp, depth)) 

# Define function for fish count based on depth
get_count <- function(depth) {
  if(depth >= 10 & depth <= 20) {
    return(sample(30:50, 1))
  } else if(depth > 20 & depth <= 35) {
    return(sample(20:30, 1))
  } else if(depth > 35 & depth <= 50) {
    return(sample(15:20, 1))
  } else if(depth > 50 & depth <= 65) {
    return(sample(0:15, 1))
  }
}

# Apply the function to the dataset
toy_data$count <- sapply(toy_data$depth, get_count)

# Remove temporary columns
toy_data <- toy_data %>%
  select(-min_temp, -max_temp)

# Sort by year, week, site, and then depth
toy_data <- toy_data %>%
              arrange(year, week, site, depth) %>%
              select(year, site, week, species, depth, temperature, count)

head(toy_data)

```


## Do Exploratory Data Analysis (EDA)

```{r}

#---------------------------------
# EXAMINE RESPONSE VARIABLE
#---------------------------------

# Histogram and density plot of response variable
hist_plot <- ggplot(toy_data, aes(x = count)) + 
  geom_histogram(color = '#FFD700', fill = '#800080', binwidth = 1) + 
  labs(title = 'Histogram of Response', x = 'Count', y = 'Frequency')

density_plot <- ggplot(toy_data, aes(x = count)) +
  geom_density(color = '#FFD700', fill = '#800080') + 
  labs(title = 'Density Plot of Response', x = 'Count', y = 'Density')

# Display plots
grid.arrange(hist_plot, density_plot, ncol = 2)

#----------------------------------------
# BAR CHARTS OF SITE, YEAR, AND SPECIES
#----------------------------------------

# Generate bar chart that sums all species for each year and site
ggplot(toy_data, aes(x = species, y = count, fill = species)) +
  geom_bar(stat = 'identity') +
  facet_grid(year ~ site) +
  theme(axis.text.x = element_blank()) +
  labs(x = 'Species', y = 'Count', fill = 'Species')


# Get each species
unique_species <- unique(toy_data$species)

# Loop through each species
for (sp in unique_species) {
  
  # Subset the data for the current species
  subset_data <- toy_data[toy_data$species == sp, ]
  
  # Create bar chart
  p <- ggplot(subset_data, aes(x = site, y = count, fill = as.factor(year))) +
    geom_bar(stat = 'identity', position = 'dodge') +
    labs(x = 'Site', y = 'Count', fill = 'Year', title = paste('Species:', sp)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 80, hjust = 1))
  
  # Print each plot
  print(p)
  
}

```


**Analysis** 

One of the things that happens with a Poisson Distribution is that as the number of
observations grows, it will increasingly move to a normal distribution. This is
because of the Central Limit Theorem (CLT). In this case, we can see that, due to the
other parameters I put on the count, the distribution is not a recognizable
distribution. The only way to accurately represent a distribution like this would be a
Gibbs Sampler or similar method, which is beyond the scope of what you are trying to
accomplish in your study. The other plots are things I would look at to try to
understand the data better. Barcharts for the different years and dive sites as well
as for each species, dive site, and year are helpful here. Your data should have much
more randomness in it, making these charts more interesting.


```{r message = FALSE}

#------------------------------------------
# CORRELATION MATRIX
#------------------------------------------

# Create correlation matrix for numeric predictors

library(corrplot)

# Select only the numeric data
numeric_vars <- toy_data %>%
                  select_if(is.numeric) %>%
                  select(-count) # excludes the response variable

# Calculate the Corr Matrix
corr_matrix <- cor(numeric_vars)

# View correlation plot
corrplot(corr_matrix, method = 'circle', type = 'lower')
corrplot(corr_matrix, method = 'number', type = 'upper', add = TRUE, 
         addCoef.col = 'black', tl.pos = 'full')

# Since 'week' and 'temperature' are 54% correlated, then you might want to examine
# their relationship more deeply
ggplot(toy_data, aes(x = week, y = temperature)) + 
  geom_point(pch = 19,
             size = 0.8,
             color = 'dodgerblue') + 
  geom_smooth(method = 'lm',
              se = TRUE,
              color = 'darkorange') +
  labs(title = 'Correlation of Week and Temp Variables',
       x = 'Week',
       y = 'Temperature')

#-------------------------------------
# SCATTER PLOTS 
#-------------------------------------

# Isolate numeric predictors
num_vars <- names(toy_data)[sapply(toy_data, is.numeric)]
num_vars <- num_vars[num_vars != 'count']  # Exclude response variable
num_vars <- num_vars[num_vars != 'year'] # Exclude year

# Create scatter plots for numeric variables
for (var in num_vars) {
  
  p <- ggplot(toy_data, aes_string(x = var, y = "count")) + 
    geom_point(pch = 19,
               size = 0.8,
               color = 'dodgerblue') +
    geom_smooth(method = 'lm',
                se = TRUE,
                color = 'darkorange') + 
    labs(x = var, y = "Count") +
    theme_minimal()
  
  print(p)
  
}


```

**Analysis** 

Not surprisingly, since we actually crafted the temperature to correspond with the
week since it is one of the variables I could look up, the two are moderately
correlated (0.51). To be sure, that is a real correlation, the only real part of all
the data.

The week and depth is also weakly correlated at 0.12 while the depth and temperature
are weakly, negatively correlated at -0.18.

As the scatter plot of the depth and count of fish shows, these two are strongly,
negatively correlated (-0.9220234).

**Key Points** 

Also, you'll notice for the count, I used a poisson distribution, this is because
Poisson is typically used to model count data such as the number of fish at a dive
site in a given year. This may prove to be helpful for you later on.

There are some models you can use that specifically utilize this distribution--Poisson
Regression, Negative Binomial Regression, Zero-Inflated Negative Binomial Regression,
and more.

Depending on how many zeros you have in your data, a zero-inflated negative binomial
regression might be appropriate because they combine a binary distribution (such as
logistic or probit regression) to model the presence/absence of an event, and a count
distribution (such as Poisson or Negative Binomial) to model the number of events
given that an event occurs.

You will have to test that out. For this particular toy dataset, I tried a Poisson
Regression and it performed very poorly and did not provide additional insight into
the data.


```{r}

# List of variables to create boxplots for
variables <- c("site", "species")

# Initialize list to store plots
plots <- list()

# Loop through each year
for (year in unique(toy_data$year)) {
  
  # Subset data for the current year
  data_year <- toy_data[toy_data$year == year, ]
  
  # Loop through each variable
  for (variable in variables) {
    
    # Create boxplot
    p <- ggplot(data_year, aes_string(x = variable, y = "count")) +
            geom_boxplot() +
            theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
            labs(title = paste("Boxplot of Count by", variable, "for Year", year))
    
    # Add plot to list
    plots[[length(plots) + 1]] <- p
  }
}

# Display plots
plots

```

**Analysis**

Again, the point of the boxplots for the categorical variables is to illustrate what
you should be doing in exploratory data analysis. The goal here is to better
understand your data. I would expect more variability within the species and sites
from year to year with a real dataset. In this instance, you could actually use a
TukeyHSD() test with a nested anova (aov()) to see if the means between the variables
are statistically significantly different. This can help you anticipate whether they
will be good predictors. For instance, if you wanted to see this in relation to
*species*, you would enter TukeyHSD(aov(count ~ species, data = toy_data)), and the
output will show you the comparison of each species of fish against the other with the
upper and lower bound for a 95% CI as well as a p-value. When I ran the Tukey HSD test
on species, every single species is statistically significantly different from the
other. 

## Mixed Effects Models (MEM)

Ok, so now you have done some exploratory data analysis and understand your data
well; You have illustrated this through the various plots. Now, let us move on to the
MEM.

One of the great secrets of data science is there is no one *obvious* choice for
analysis This is why we will try several models to see which one produces the lowest
Mean Squared Error (MSE). The MEM has some unique challenges because the fixed
effects and random effects are not obvious.

Fixed effects are the coefficients that are estimated in a regression model and
represent the average effect a predictor has on the response variable. They are also
typically the variables of primary interest and are usually factors you manipulate or
control in your study. Random effects, on the other hand, are random variables
representing effects that are specific to certain units of observation and are
assumed to come from a population with a normal distribution. They are the factors
that you *suspect* influence the response variable but you do not necessarily
control. In your case, as you mentioned to me, the fixed effects would most likely be
the *year* and *dive sites*, since those are things under your control; you do not
have much control over any of the other factors. There is a way to test for the best
model, which we will use later.

You will recall, one of the assumptions that must be met for a multiple linear
regression (MLR) model is that the *observations* are independent, whereas a mixed
effects model assumes the *errors* are independent. The year to year comparison is
likely not fully independent, neither are the dive sites, nor the number of fish in
warmer water vs colder water, nor the number of fish at different depths. All of
those are likely *dependent*. Therefore, a MEM is actually more appropriate here than
the MLR. Longitudinal data is precisely the type of data that MEMs are applied to. 

Since the mixed effects model is a function of the fixed and random effects, the
fixed effects are regression coefficients that are estimated, while the random
effects are random variables that add an additional source of variance to the model.
The mixed effects model is given by the following equation:

$$ Y = X\beta + Zb + \epsilon$$

where $Y$ is the response variable (i.e. *count*), $X$ is the matrix of predictors
(i.e. independent variables) associated with the fixed effects, and $\beta$ is the
vector of the fixed-effects parameters (i.e. coefficients). Similarly, $Z$ is the
matrix of predictors associated with the random effects and $b$ is the vector of
random-effects parameters (i.e. coefficients that quantify the variability of the
relationship between the predictors in the $Z$ matrix and the response variable $Y$).
In this case, the coefficients identified in the $b$ vector quantifies how much the
effect varies across dive sites and years. Finally, $\epsilon$ is the vector of
errors, also known as the residuals, which are the difference between the actual
values of the response variable and the values predicted by the model.

With all that said, let us move to implementing a MEM. 

First, we will set it up to consider the year and site as an interaction term, like
you suggested. There is already research in your field (cf. Krysiak, F. (2021).
*Linking human impacts to recent declines in coral reef fish communities in the Bay*
*Islands*) that has used this to great effect. By using the interaction term between
'year' and 'site', you are acknowledging that the effect of 'year' on the count of
the fish sometimes varies based on the site, and vice versa. The research by Krysiak
(ibid.) focused on the biomass of the fish species, which was an interesting way to
consider it. Two large fish of a species would contribute more to the biomass than
several small fish of another species. It also looks like Krysiak's research was
focused on the Bay Islands, whereas I think you told me you were focused on four
different dive sites rather than four different islands in the Bay Islands.  

We must first discover the best *null model*. Unfortunately, there is no *right*
answer to this problem as it is subjective to your understanding of your data. I am
shooting blind here and just developing a few so that you can see how you would go
about building the null models, which are constructed with the random effects that
you think exist within your data and might influence the response variable. First,
some description.

If there is a 1 before the line, this suggests random intercepts exists for the
variable in consideration; whereas a 0 suggests the variable will have a fixed
intercept.

For the hypothetical null models I made, here is how to understand them.

**(1|depth)**: Suggests there are random intercepts and slopes for depths.
**(1 + depth|species)**: Surmises that the depth will vary between species of fish. 
**(1|depth) + (1|temperature)**: Specifies that there are random intercepts for both
depth and temperature.
   * Special note: It is unusual to have temperature as a random effect. Typically,
   random effects are when there are several observations from various groupings and 
   you think there might be a group effect. In this case, temperature will vary 
   between depths and the depths are set intervals, rather than continuous (i.e. 0 - 
   65). 
**(1 + depth*temperature|species)**: Each species might have their intercepts and 
random slopes influenced by both depth and temperature. Moreover, their slopes and
intercepts can be correlated.
**(1|temperature) + (1 + depth | species)**: Again, the 1|temperature indicates
random intercepts for temperature. (1 + depth | species) indicates that there will be
a random intercept and slope for depth within each level of species.


```{r}

# Load required packages
library(lme4)
library(nlme)
library(lmerTest) #This package modifies the lme4 package so that the summary 
                  #output shows the p-values of the coefficients

# One of the first things you can do is see which random effect term(s) best fits the data.
h0m1 <- lmer(count ~ 1 + (1|depth), data = toy_data, REML = FALSE)
h0m2 <- lmer(count ~ 1 + (1 + depth|species), data= toy_data, REML = F)
h0m3 <- lmer(count ~ 1 + (1|depth) + (1|temperature), data = toy_data, REML = F)
h0m4 <- lmer(count ~ 1 + (1 + depth*temperature|species), data = toy_data, REML = F)
h0m5 <- lmer(count ~ 1 + (1|temperature) + (1 + depth | species), data = toy_data, REML = F)

anova(h0m1, h0m2, h0m3, h0m4, h0m5)

```

Data: toy_data
Models:
h0m1: count ~ 1 + (1 | depth)
h0m3: count ~ 1 + (1 | depth) + (1 | temperature)
h0m2: count ~ 1 + (1 + depth | species)
h0m5: count ~ 1 + (1 | temperature) + (1 + depth | species)
h0m4: count ~ 1 + (1 + depth * temperature | species)

|**model**|**npar**|**AIC**|**BIC**|**logLik**|**deviance**|**Chisq** |**Df**|**Pr(<Chisq)**            |
|:--------|-------:|------:|------:|---------:|-----------:|---------:|-----:|-------------------------:|
|h0m1     |       3|29354  | 29373 | -14674   | 29348      |          |      |                          |
|h0m3     |       4|29353  | 29378 | -14672   | 29345      | 3.1661   | 1    |   0.07518                |
|h0m2     |       5|21997  | 22028 | -10994   | 21987      | 7357.6671| 1    | < 0.0000000000000002 *** |
|h0m5     |       6|21983  | 22020 | -10985   | 21971      | 16.4339  | 1    |   0.00005038 ***         |
|h0M4     |      12|22018  | 22093 | -10997   | 21994      | 0.0000   | 6    |   1.00000                |

**Analysis**

In the hypothetical scenario here, the fifth null model, *h0m5*, had the most improvement, 
as seen by the improvement of Bayes Information Criterion (BIC), Akaike Information 
Criterion (AIC), and the p-value. While h0m5 is the best in the absolute sense with the BIC and AIC,
it is important to recognize that the h0m2 had dramatic improvement over h0m3 and h0m1 as well. h0m5 
just had the *best* performance with the evaluation criteria we are using. The goal is to show how 
I would evaluate the null models. Remember, the null models are wholely dependent upon your 
understanding of your data. There is not a "right" answer, per se. 

### Fit Mixed Effects Model

Now that we have compared five null models with varying random effect structures and
found which one produces the best fitting model, we can move on to the full model. 

Importantly, we create an interaction term with 'year*site' that enables the effect
of the 'year' on the 'count' to vary depending the dive 'site.' If the interaction
term is statistically significant, you can conclude that the effect of the year on
the response variable 'count' is not identical at each dive site. Put differently,
the yearly change in the count of fish varies between dive sites. However, if the
interaction term is not statistically significant, then we can conclude that the
effect of 'year' on the 'count' does not vary significantly between dive sites.

When you use **'*'** in R in this context, it is shorthand for including both the
main effects of 'year' and 'site' and their interaction effect. Therefore,
'year*site' is equivalent to 'year + site + year:site'. 


```{r}

# Create object to store MSE values in
MSEtest <- NULL

# Fit the model
me_model <- lmer(count ~ year*site + week + (1 | temperature) + (1 + depth | species), data = toy_data)

# Show the summary
summary(me_model)

# Calculate the fitted values for the fixed and random effects
fitted_random <- fitted(me_model, type = "response")
fitted_fixed <- fitted(me_model, type = "fixed")

# Calculate the total variance
total_var <- var(me_model@resp$y)

# Calculate the variance of the fitted values
random_var <- var(fitted_random)
fixed_var <- var(fitted_fixed)

# Calculate R^2
R2_m <- fixed_var / total_var
R2_c <- random_var / total_var

# Print the results
cat('Marginal R-Squared is ', R2_m, '\n',
    'Conditional R-Squared is ', R2_c, '\n', sep = "")

# Examine the residual plot
mem_resids <- residuals(me_model)

plot(predict(me_model), mem_resids,
     xlab = 'Fitted Values', 
     ylab = 'Residuals',
     main = 'Residual Plot',
     pch = 19,
     col = 'dodgerblue',
     cex = 0.8)
abline(h = 0, lty = 2, col = 'red', lwd = 2)


# Create random and fixed effects objects
random_effects <- summary(me_model)$varcor %>%
                   data.frame() %>%
                   rename('Group' = 'grp',
                          'Variance' = 'vcov',
                          'Stdev' = 'sdcor',
                          '_' = 'var1')

fixed_effects <- summary(me_model)$coeff %>%
                  data.frame()

# View Random And Fixed Effects
random_effects[, -3]

fixed_effects

# If you want to view which level is your reference level, use the 
# contrasts() method from the standard stats package. You can also 
# manually set the reference group if that is important to your research
# goal

contrasts(toy_data$site)
contrasts(toy_data$species)

me_MSE <- me_model@sigma^2

MSEtest <- c(MSEtest, me_MSE)

#--------------------------------------
# COMPARE NULL MODEL AND FULL MODEL
#--------------------------------------

# Create null model to compare to the full model
null_mod <- lmer(count ~ 1 + (1 | temperature) + (1 + depth | species), data = toy_data)

# Compare null and full model
anova(me_model, null_mod)

# Calculate the fitted values for the fixed and random effects
fitted_random_null <- fitted(null_mod, type = "response")
fitted_fixed_null <- fitted(null_mod, type = "fixed")

# Calculate the total variance
total_var_null <- var(null_mod@resp$y)

# Calculate the variance of the fitted values
random_var_null <- var(fitted_random_null)
fixed_var_null <- var(fitted_fixed_null)

# Calculate R^2
R2_m_null <- fixed_var_null / total_var_null
R2_c_null <- random_var_null / total_var_null

# Compare the two models R^2 and MSE
cat('Full Model', '\n',
    'R-Squared: ', R2_m, '\n',
    'MSE: ', me_model@sigma^2, '\n\n',
    'Null Model', '\n',
    'R-Squared: ', R2_m_null, '\n',
    'MSE: ', null_mod@sigma^2, sep = "")

```

**Analysis**

This toy dataset actually has an $R^2$ of 87.79627, meaning about 88% of the variance
in the count of fish can be described by the predictors in the model. Not a single
predictor in my model is statistically significant at an $\alpha$-value of 0.05.
Nevertheless, even when this is the case, it does not mean that the model lacks
explanatory power. From the random effects, *species* explains a tremendous amount of
the variance in the model, even though we do not have a specific coefficient since we
used it as a part of the random effects. 

  * Null Vs Full Model Comparison

The null model is just the model with the random effects and the intercept and no
other predictor variables. It is important to compare the model you create with a
null model to see if the full model provides additional explanatory power as compared
to if you did not have these additional predictors at all. The null hypothesis you
are testing against is:

$H_0$: The simpler model (i.e. null model) is as good as the full model.

The p-value from the ANOVA comparison of the models is 0.9181, which means we would
fail to reject the null hypothesis that the simpler model is as good as the full
model. When I reran my model with just the null model, the MSE is 16.68427 vs
16.74543 for the full model. In this case, it produces slightly better results,
confirming the ANOVA test. Nevertheless, I would be comfortable keeping the full
model. 

The residuals plot can help us assess goodness of fit. In an ideal world, the
residuals should be randomly distributed around zero, with no obvious pattern to
them. Since I created this dataset, there are patterns to it in the residuals plot.
While the residuals are somewhat evenly distributed around zero, they are still
grouped together, which might show some correlation or autocorrelation. In the
context of time-series data, patterns in the residuals sometimes suggest that the
current obervations depend on the previous ones (i.e. autocorrelation). What we are
looking for in the residual plot is homoscedasticity, meaning that the residuals are
spread across all levels of the predicted values consistently. 

When we have heteroscedasticity, it can be appropriate to do some form of
transformation on your response variable, which is often done through a BoxCox
transformation. I will not do that with the toy dataset here.

## Multiple Linear Regression (MLR)

The null hypothesis of the MLR is that the true coefficients for each of the
predictors is zero, meaning that the predictors have no effect on the response
variable of the count of fish.

$H_0$: The value of all coefficients for the predictors are zero.

$H_a$: The value of at least one of the coefficients for the predictors is *not*
zero.

Therefore, if a p-value for the model is less than the $\alpha$-value of 0.05, then
we would reject the null hypothesis that all the coefficients for the predictors are
zero and conclude that at least one of them is not-zero and has *some* effect on the
response variable.

The other benefit of the linear regression model in R is that it can handle both
categorical and numeric predictors without having to convert the categorical
variables to dummy variables. 

MLR is characterized by the following equation:

$$ y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$$

where Y is the response variable, $\beta_0$ is the intercept, 
$\beta_1, \beta_2 ... \beta_p$ are the regression coefficients that represent the
average change in the estimated value of $Y$ for every one-unit change in the
corresponding predictor variable, *assuming all other variables are held constant*,
$X_1, X_2 ... X_p$ are the predictor variables, and $\epsilon$ is the error term
(i.e. residual). 

The goal of the multiple linear regression is to minimize the sum of the squared
residuals, also referred to as the Ordinary Least Squares (OLS). 

```{r}

# Split the data into training and test sets
training_samples <- createDataPartition(toy_data$count, p = 0.8, list = FALSE)

train_data <- toy_data[training_samples, ] 
test_data <- toy_data[-training_samples, ]

```

**Conduct Linear Regression**

```{r}

linreg_mod <- lm(count ~., data = train_data)

summary(linreg_mod)

fstat <- summary(linreg_mod)$fstatistic
pvalue <- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)

# Make prediction on test data
pred_lr <- predict(linreg_mod, test_data)

# Calculate MSE for models
lr_MSE <- mean((pred_lr - test_data$count)^2)

test_data_r2 <- R2(pred_lr, test_data$count)

cat('The Adj R-Squared for the linear regression model is ', round(summary(linreg_mod)$adj.r.squ, 6), '\n', 'The p-value for the model is ', pvalue, '\n', 
    'The MSE of the LR model is ', lr_MSE, sep = "")

MSEtest <- c(MSEtest, lr_MSE)

#-----------------------------
# RESIDUALE VS FITTED VALUES
#-----------------------------

ggplot(data = train_data, aes(x = linreg_mod$fitted.values, y = linreg_mod$residuals)) + 
  geom_point(alpha = I(0.4), color = 'darkorange') + 
  xlab('Fitted Values') + 
  ylab('Residuals') + 
  ggtitle('Residual Plot') + 
  geom_hline(yintercept = 0)


#------------------------------
# HISTOGRAM AND QQPLOT 
#------------------------------

hist_plot = ggplot(train_data, aes(linreg_mod$residuals)) + 
              geom_histogram(binwidth = 1, 
                             fill = '#800080',
                             color = '#FFD700') +
              labs(x = 'Residuals',
                   y = 'Count',
                   title = 'Histogram of Residuals') + 
              theme_minimal()

qqp = ggplot(train_data, aes(sample = linreg_mod$residuals)) +
          stat_qq(alpha = I(0.2), 
                  color = 'darkorange') + 
          labs(title = 'Q-Q Plot of Residuals',
               x = 't Quantiles',
               y = 'Studentized Residuals')


ggarrange(hist_plot, qqp, ncol = 2, nrow = 1)


```

**Analysis**

In this linear regression, we made no interaction terms, but rather just used all
predictors as is (i.e. count~.,). In my toy data, we see that the week of the year,
all the species, and the depth are all statistically significant variables. To be
clear, this is not the best way to select variables as there are other ways to deal
with that. The p-value for the model is effectively 0.000, meaning that the model is
statistically significant and we reject the null hypothesis that the value of all the
coefficients is zero. In other words, at least one of the variables has explanatory
power and is significantly non-zero. 

In your data, I would expect the *year* variable to have a more significant effect on
the response variable. We see that the Adjusted $R^2$ for the linear regression model
is 0.863513 and the MSE is 23.83673, both of which are worse than the mixed effects
model. 

The residual plot checks for Independence (i.e. makes sure that the errors are
uncorrelated) and Constant Variance (homoscedasticity).  Based on the residual plot,
similar to the Mixed Effects Model (me_model), the residuals show some form of
heterscedasticity. It would be customary to explore this more and likely do some form
of transformation on the response variable. 

The histogram and qqplot check for normality. In this case, our residuals are
wonderfully symmetrical and therefore uphold the assumption of normality. 

### Generalized Additive Model (GAM)

The GAM is a statistical model that allows for non-linear relationships between the
predidctors and the response variable. It is given by the following equation:

$$g(E[Y]) = \beta_0 + f_1(X_1) + f_2(X_2) + \ldots + f_p(X_p)$$

where $E[Y]$ is the expected value of the response variable, $X_1, X_2, ... X_p$ are
the predictor variables, $g()$ is the link function which relates the expected value
of the response variable to the predictor variables, $\beta_0$ is the intercept, and
$f_1(), f_2(), ... f_p()$ are smooth, non-linear function of the predictors.

Not to get too far into the weeds, but it is important to understand, the GAM
function in R uses as its default smoothing function what are called "thin plate
splines." There are many different smoothing functions you could deploy. Here, we
just use the default. Thin plate splines fit a smooth surface to a numeric predictor
such that the data is as smooth as possible. In the model below, each numeric
variable is wrapped in s(). 'Year' is treated as a factor variable, and therefore is
not wrapped in s(). s() in this case indicates the smoothing function.


```{r}

# Fit model
gam_model <- gam(count ~ year + site + s(week) + species + s(depth) + s(temperature), data = train_data)

summary(gam_model)

# Make predictions on the test data
pred_gam <- predict(gam_model, newdata = test_data)

# Calculate the MSE
gam_MSE <- mean((test_data$count - pred_gam)^2)

# R^2
r2_gam <- 1 - (gam_model$deviance/gam_model$null.deviance)

# Show MSE
cat('Model (GAM):\nMSE = ', gam_MSE, '\n',
    'R-Squared = ', r2_gam, '\n\n', sep = "")

MSEtest <- c(MSEtest, gam_MSE)

```

**Analysis**

The MSE for this model is 19.86183, which still performs worse than the mixed effects
model. The adjusted $R^2$ is 0.886571, which indicates that about 88.7% of all the
variance in the response variable can be explained by the predictors used in the
model. The GAM provides a section for the significance of the smoothing terms (the
quantitative variables), and suggests that the *depth* variable is statistically
significant.

### Final Model Comparison

```{r}

MSEtest %>%
  cbind(c(R2_m, summary(linreg_mod)$adj.r.squ, r2_gam)) %>%
  data.frame(row.names = c('Mixed Effects', 'LinReg', 'GAM')) %>%
  rename('MSEs' = '.',
         'R2s' = 'V2')

```

Comparing the MSEs of each model shows that that the mixed effects model does a
better job of modeling the data than the other two models. Ultimately, minimizing the
MSE is more important to me than a sligthly improved $R^2$. The $R^2$ is slightly
better with th GAM but the MSE is worse. If I were choosing which model to use, I
would choose the Mixed Effects Model. Your real data may reveal something different,
but that is what I would do with this toy dataset. 
