---
output:
  word_document: default
  html_document: default
---
# Bookings Analysis

**Commissioned By**: Director of Advising

The Director of Advising asked if we could analyze the bookings appointments and do a comparative analysis. We incidentally have two
groups, a control group and an experimental group; this sets up what is referred to as a *natural experiment*. The control group are the
students who did not have a follow up email or phone call after they did not enroll or did not show up for an appointment. The way I am 
initially thinking of setting up the data is to have the following predictors:

    * Month: Extracted from the Date Time column.
    * Staff: Advisor Signature
    * Student_Type: Type of student (Current, High School, New)
    * num_of_contacts Number: Number of times the student was contacted, which will range from 0 to 3 and be calculated by counting the instances from "SPACMNT Notes," "Second Contact," and "Third Contact." If there are notes in all three columns, then the student was contacted three times. If there are only notes in two of them, then they were only contacted twice. If only in one, then contacted once. 
    * Semester_Enrolled: Semester student indicated they were enrolling for, and what the "Enrolled" column is based on.

**Response**

    * Enrolled: This is the response variable. It indicates whether the student is enrolled in the semester they indicated or not.
  
## The Data

The data is pulled from the Bookings forms stored in the Advising Teams website under Academic Advising > Advising Bookings Appointments >
Files > Archive > 2022 Bookings Appointments.xlsx and under Files > 2023 Bookings Appointments.xlsx, 2024 Bookings Appointments.xlsx. When
combined, there are 7046 rows (i.e. appointments), 6 predictors and 1 response variable, all mentioned above. 

The response variable--"Enrolled"/"Not Enrolled"--had several different iterations. 

    * Enrolled
    * yes
    * Yes
    * enrollment
    * yes-summer
    * yes-fall
    * yes & yes
    
These were all the affirmative versions. Therefore, if a student appointment had any of these responses, they were changed to (or kept as)
"Enrolled" with all others stored as "Not Enrolled." 

There were some anomalies in a few of the columns. First, there was one in the "staff" column that was signed off as "DR, JW". That row
was eliminated. There was another row saved as "C" in the "staff" column. That row was deleted. In the "semester_enrolled" column, there
was one row that had a "0" stored and another that had a "yes" stored. Those were both deleted. Altogether, four rows out of 7046 rows.
Therefore, their removal was inconsequential. Finally, in the *semester_enrolled* column, there were several instances in which a student
was indicated to have enrolled in multiple semesters. There was 31 instances of "Spring and Summer," 263 instances of "Summer and Fall",
and 21 instances of "Spring late start. All of these were converted to the closest semester. Meaning "Spring and Summer" was converted to
"Spring," "Summer and Fall" was converted to "Summer" and "Spring late start" was converted to "Spring." This effects 4.47% of the
observations.

### Feature Engineering

A few features were extracted from existing parts of the data or were created by combining different elements of the data. The year and
month were pulled from the original "Date Time" column. The "student_type" column was created by pulling the student type out of the
original "service" column in the native data set. Finally, the "num_of_contacts" was generated by looking into the "second contact" and
"third contact" columns and creating a count based on what was in those columns. The default is one, because we assume that every student
was contacted the day of their appointment. 
    
```{r include = FALSE}

# clear objects
rm(list = ls())

# for reproducability
set.seed(101)

# Set to non-scientific notation
options(scipen = '0')

# set working directory
setwd('C:/Users/aslemp/Desktop/pandas/Butler Programming/Bookings Analysis/')

# Load libraries
  
library(tidyverse)
library(ggplot2)
library(glmnet)
library(caret)
library(ggpubr)
library(e1071)
library(pROC)

# Load dataset
bookings <- read_csv('Bookings Dataset For Analysis.csv')

dim(bookings)

## [1] 7046    7

summary(bookings)

table(bookings$enrolled) %>% 
  cbind(table(bookings$enrolled) / sum(table(bookings$enrolled))) %>%
  data.frame() %>%
  rename('num_enrolled' = '.',
         'perc_enrolled' = 'V2')

# Make every categorical column a factor. The month column, while numeric, is a numeric rep-
# representation of the categorical values. 'num_of_contacts' was left out as that
# is supposed to be numeric. Since it is the only numeric value, it does not need
# to be scaled.

categorical <- c("month", "year", "staff", "student_type", "semester_enrolled",
                 "enrolled")

for(i in categorical){
  
  bookings[[i]] <- as.factor(bookings[[i]])
  
}

# Remove a single instance of a signature having 'DR, JW'
# And three instances of 'C', one instance of '0' in semester_enrolled and 
# one instance of 'yes' in semester_enrolled.
bookings <- bookings %>%
            filter(staff != 'DR, JW',
                   staff != 'C',
                   semester_enrolled != '0',
                   semester_enrolled != 'yes',
                   staff != 'SP')


# Clean up the instances in which there are two semesters of enrollment listed that 
# the student enrolled in. I switched it to be whatever the closest semester was
bookings$semester_enrolled[bookings$semester_enrolled == 'Spring and Summer'] <- 'Spring'
bookings$semester_enrolled[bookings$semester_enrolled == 'Summer and Fall'] <- 'Summer'
bookings$semester_enrolled[bookings$semester_enrolled == 'Spring late start'] <- 'Spring'

# Check one more time to see what unique values are left. 
unique(bookings$semester_enrolled)

```

## Exploratora Data Analysis

### Bar Charts

Using bar charts to visualize how the numeric predictor interacts with the response variable.

```{r}


for(i in c(2022, 2023, 2024)){
  
  temp <- bookings[which(bookings$year == i), ]
  
  p = ggplot(temp, aes(x = num_of_contacts, 
             fill = enrolled)) +
             geom_bar(position = 'dodge') + 
             scale_fill_manual(values = c("#6A5ACD", "#FFD700"), 
                               labels = c("Persisted", "Did Not Persist"),
                               name = NULL) + 
             labs(title = paste('Num Of Contacts And ReEnrollment In ', i),
                  y = 'Headcount',
                  x = 'Num of Contacts') + 
             theme(plot.title = element_text(size = 12),
                   axis.title.x = element_text(size = 10),
                   axis.title.y = element_text(size = 10))
  
  t <- table(temp$num_of_contacts, temp$enrolled) %>%
            cbind(prop.table(table(temp$num_of_contacts, temp$enrolled), 1)) %>%
            data.frame() %>%
            rename('Enrolled_%' = 'Enrolled.1',
                   'Not_Enrolled_%' = 'Not.Enrolled.1')

  print(p)
  print(t)
  
}


```

**Analysis of Bar Charts**

If we go back to 2022, we see that 95.35% of the students who set up an appointment were enrolled by the end of that appointment. This
means that just under 5% (4.65%) were left unenrolled. That means the additional contacts are chasing those four or five percent. out of
those 5%, 45.62% will eventually get enrolled, which in 2022 was an additional 595 students across all semesters (Fall, Spring, Summer).
Read on before you get too excited and see the odds of a student enrolling based on additional contacts.

2023 saw a dip in first contact enrollments. 91.95% of students enrolled by the end of their first contact appointment. The shocker for
2023 is that 8.05% of students did not enroll after their first contact, which is 73.12% more students who remained unenrolled. Out of the
8.05% that did not enroll, 45.14% of them eventually enrolled. Again, this is across all semesters--Fall, Spring, Summer. 

And so far, for 2024, out of the appointments that have been made, 93.37% of those students are enrolled and 6.63% are not enrolled. 

### Chi-Square Tests

The $\chi^2$ test of independence helps to ascertain whether there is a significant association between two separate categorical
variables. The $\chi^2$ test is given by:

$$\chi^2 = \sum \frac{(O_{ij}-E_{ij})^2}{E_{ij}}$$
where $O_{ij}$ represents the frequency of each observation $i$ from each row and each attribute from the $j^{th}$ column, $E_{ij}$
represents the expected frequency of each observation, $i$, from each row and each attribute from the $j^{th}$ column.

When applied to a binary classifier and a categorical predictor, the test reveals whether the observed distribution of the response
variable is independent of the predictor under consideration. The null hypothesis is:

$H_0$: The two categorical variables are independent.

while the alternative hypothesis is:

$H_a$: The two categorical variables are statistically significantly associated or dependent.

When the $\alpha$ value is set to 0.05, if the p-value for the $\chi^2$ test is less than that, then the null hypothesis is rejected and
one can conclude that there is a statistically significant association between the response and the categorical variable it is compared to
in the test. 

```{r include = FALSE}

# month
chisq.test(table(bookings$month, 
                 bookings$enrolled))

# year
chisq.test(table(bookings$year, 
                 bookings$enrolled))

# staff
chisq.test(table(bookings$staff, 
                 bookings$enrolled))

# student_type
chisq.test(table(bookings$student_type, 
                 bookings$enrolled))

# semester enrolled
chisq.test(table(bookings$semester_enrolled, 
                 bookings$enrolled))

# num_of_contacts 
chisq.test(table(bookings$num_of_contacts, 
                 bookings$enrolled))


```


**Analysis of $\chi^2$ Test**

For the construction of the models, *year* is not necessary to keep as it is not significantly associated with the response variable. The
$\chi^2$ test has a p-value of 0.2546, indicating there is not a statistically significant association. This should not be surprising.
There is a statistically significant association between the variables *month*, *staff*, *student_type*, *semester enrolled*, and
*num_of_contacts* and the response variable *enrolled*. Therefore, we will keep all of those for the analysis.

## Model Building

The data are split into a training and test set, 80% for training and 20% for test.

```{r include = FALSE}

training.samples <- bookings$enrolled %>%
                     createDataPartition(p = 0.8, list = FALSE)

train.data <- bookings[training.samples, ]
test.data <- bookings[-training.samples, ] 

```

### Logistic Regression

A well known form of regression analysis for binary classification is logistic regression. One of its core assumptions is that the data
can be separated into roughly equal portions of both outcomes, 1 or 0, True or False, or in the case of this analysis, Enrolled or Not
Enrolled. The model is predicated on the logistic function, also called the sigmoid function, $\sigma(z)$, and is defined as:

$$\sigma(z) = \frac{1}{1+e^{-z}}$$

where $z$ is the input of the function and $e$ is the base of the natural logarithm. $z$ is the linear combination of the input features
$X$ and the model parameters $\beta$, which is given by the following equation:

$$z = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_nX_n$$

The $\beta_0, \beta1, \dots, \beta_n$ are the model parameters, and $X_1, X_2, \dots X_n$ are the feature values for each observation. 

The logistic function transforms the linear output into a value between 0 and 1, which can be interpreted as the probability $p$ of the
positive class:

$$p = \sigma(\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_nX_n)$$
The result of this calculation is then used to classify an observation into the positive or negative class via a simple heuristic of using
a threshold of 0.5. If the predicted probability is greater than 0.5, then the prediction is rounded to 1, and if not, it is rounded to 0.
To be clear, this threshold is not a fixed number and can be altered by the modeler if the cost of false positives or false negatives is
of particular import. 

To fit the parameters of $\beta$ to the training dataset, this model *typically* uses a maximum likelihood estimator (MLE) to uncover the
values of $\beta$ that maximize the likelihood of observing the classification labels ('Enrolled', 'Not Enrolled') given the features $X$
and parameters $\beta$. 

Using the *enrolled_mod* dataset, run a logistic regression.

```{r include = F}

M1 <- glm(enrolled~., family = binomial(link = 'logit'), data = train.data)

rownames(round(summary(M1)$coeff[,c(1,4)], 6) %>%
  cbind() %>%
  data.frame() %>%
  filter(Pr...z.. <= 0.05) %>%
  rename('P_Val' = 'Pr...z..'))

summary(M1)

M1_pred <- predict(M1, test.data, type = 'response')
  
M1_pred <- ifelse(M1_pred >= 0.5, 'Not Enrolled', 'Enrolled')
  
confusionMatrix(table(M1_pred, test.data$enrolled))

#---------------------------------------------------
# Mccv With LR
#---------------------------------------------------

B = 20

lr_acc = NULL
lr_acc_pval = NULL
lr_spec = NULL
lr_sens = NULL

for(i in 1:B){
  
  training.samples <- bookings$enrolled %>%
                     createDataPartition(p = 0.8, list = FALSE)

  train.data <- bookings[training.samples, ]
  test.data <- bookings[-training.samples, ] 
  
  lr_model <- glm(enrolled~., family = binomial(link = 'logit'), data = train.data)

  lr_pred <- predict(lr_model, test.data)
    
  lr_pred <- ifelse(lr_pred >= 0.5, 'Not Enrolled', 'Enrolled')
  
  lr_acc <- rbind(lr_acc, cbind(confusionMatrix(table(lr_pred, test.data$enrolled))[[3]]['Accuracy'][[1]]))
  lr_acc_pval <- rbind(lr_acc_pval, cbind(confusionMatrix(table(lr_pred, test.data$enrolled))[[3]]['AccuracyPValue'][[1]]))
  lr_spec <- rbind(lr_spec, cbind(confusionMatrix(table(lr_pred, test.data$enrolled))[[4]]['Specificity'][[1]]))
  lr_sens <- rbind(lr_sens, cbind(confusionMatrix(table(lr_pred, test.data$enrolled))[[4]]['Sensitivity'][[1]]))
  
}

# Output of accuracy, specificity, and accuracy p-value
rbind(cbind(round(mean(lr_acc), 6), round(var(lr_acc), 6)),
cbind(round(mean(lr_acc_pval), 6), round(var(lr_acc_pval), 6)),
cbind(round(mean(lr_spec), 6), round(var(lr_spec), 6)),
cbind(round(mean(lr_sens), 6), round(var(lr_sens), 6))) %>%
  data.frame(row.names = c('Accuracy', 'Accuracy_P_Value', 'Specificity', 'Sensitivity')) %>%
  rename('Avg' = 'X1',
         'Avg_Var' = 'X2')

```

#### Model Significance

```{r include = FALSE}

# Is the model significant at a p-value of 0.01
1 - pchisq((M1$null.deviance - M1$deviance), 
                  (M1$df.null - M1$df.residual))

```

**Analysis of model significance**

The model is statistically significant, which simply means it is better than guessing. 

#### Examine Fitted Vs Residuals

```{r include = F}

# Fitted vs residuals plot

plot(M1$fitted.values, M1$residuals,
     main = 'Fitted vs. Residuals',
     xlab = 'Fitted Values', 
     ylab = 'Residuals',
     pch = 19,
     cex = 0.8,
     col = 'blue')

# Pearson Residual Plot
pears_res <- resid(M1, type = 'pearson')

plot(M1$fitted.values, pears_res,
     main = 'Fitted vs. Residuals',
     xlab = 'Fitted Values', 
     ylab = 'Residuals',
     pch = 19,
     cex = 0.8,
     col = 'blue')

# Deviance Residual Plot vs Fitted Values
dev_resid <- resid(M1, type = 'deviance')

plot(M1$fitted.values, dev_resid,
     main = 'Fitted vs. Residuals',
     xlab = 'Fitted Values', 
     ylab = 'Residuals',
     pch = 19,
     cex = 0.8,
     col = 'blue')

# Histogram of Deviance Residuals
hist(dev_resid, 
     main = "Histogram of Deviance Residuals", 
     xlab = "Deviance Residuals")

# QQplot of deviance residuals
qqnorm(dev_resid)
qqline(dev_resid)

# Leverage and Inflence Points (i.e. outliers)
M1_cooks <- cooks.distance(M1)

```

**Analysis of Goodness of Fit**

The QQplot and histogram of the deviance residuals shows a roughly normal distribution, which suggests the model is a good fit. However,
we can apply the actual deviance and pearson residual tests to see if the model is statistically significantly a good fit. 

### LR Goodness of Fit Tests

To test whether the logistic regression model is a good fit, two separate tests should be run--the deviance residuals test and the Pearson
residuals test. Both have the same null hypothesis.

$H_0$: The model *is* a good fit.

$H_a$: The model *is not* a good fit.

This is one of the rare times the researcher is hoping for a large p-value because a large p-value would indicate that the model fits the
data reasonably well. A small p-value, one under the traditional $\alpha$-value of 0.05, would mean the researcher rejects the null
hypothesis that the model is a good fit and instead conclude that the model is not a good fit for the data. 

```{r include = F}

library(ResourceSelection)

#Deviance
dev <- 1 - pchisq(M1$deviance, M1$df.residual)

#Pearson residuals
pear_resid <- resid(M1, type = 'pearson')
pears <- 1 - pchisq(sum(pear_resid^2), M1$df.residual)


```

```{r}

cat('The Deviance residuals p-value is ', dev,', and the Pearson ', '\n',
    'residuals p-value is ', pears, '. Therefore, we fail to reject the ', '\n',
    'null hypothesis that the model is a good fit for the ',
    'data.\nThus concluding it is a good fit.', sep = "")

```
#### Examine The Significant Variables

```{r include = F}

summary(M1)

exp(summary(M1)$coeff['num_of_contacts', 'Estimate']) - 1

contrasts(bookings$month)

# Set Different Reference Level
bookings$month <- relevel(bookings$month, ref = '5')

contrasts(bookings$month)

```


#### Evaluate Month

The reference level for the months is January (1).

First, the following months are statistically significantly non-zero at an $\alpha$-level of 0.05.

April, May, June, July, August, October, and November.

    * The odds a booking student will enroll increases by 41.45% during the month of April vs January.
    * The odds a booking student will enroll increases by 65.69% during the month of May vs January.
    * The odds a booking student will enroll increases by 60.17% during the month of June vs January.
    * The odds a booking student will enroll increases by 66.98% during the month of July vs January.
    * The odds a booking student will enroll increases by 66.01% during the month of October vs January.
    * The odds a booking student will enroll increases by 66.10% during the month of November vs January.

With June as the reference level, the odds of enrollment are shown below.

    * The odds a booking student will not enroll increases by 144.72% during the month of January vs June.
    * The odds a booking student will not enroll increases by 79.76%% during the month of February vs June.
    * The odds a booking student will not enroll increases by 241.05% during the month of March vs June.
    * The odds a booking student will not enroll increases by 97.66% during the month of December vs. June.

With August as the reference level, the odds of enrollment are shown below.

    * The odds a booking student will not enroll increases by 130.39% during the month of March vs August.
    * The odds a booking student will enroll increases by 44.12% during the month of July vs August.
    * The odds a booking student will not enroll increases by 180.37% during the month of September vs August.
    
What does this all mean? I compared January with the other months to look at enrollment trends for Spring, June to the early months of the
year to look at the summer enrollment, and August to the months between March and August to look at enrollment trends for Fall. For
summer, the only statistically significant non-zero coefficients were January, February, March, and December. I included December because
that is when enrollment for Summer typically begins. For Fall, the only statistically significant non-zero coefficients were March, July,
and September. I included September because students are still enrolling in Fall at that time but are not yet able to enroll in Spring or
Summer.

**For Fall**

Booking appointments made for Fall enrollment during the month of March are 130% less likely to end in an enrollment than a booking
appointment made in August. Similarly, a booking appointment made in July for Fall enrollment is 44% less likely to end in an enrollment
than one made in August. Students who make a booking appointment in September are 180% less likely to enroll in a Fall class than students
who make appointments in August. 

**For Spring**

Booking appointments made in October, when Spring enrollment begins, are 66.01% more likely to end in an enrollment than a booking
appointment made in January. Almost identically, students who set a booking appointment in November are 66.10% more likely to enroll than
a student who sets a booking appointment in January. All other months of Spring enrollment were not statistically significant.

**For Summer**

Students who make a booking appointment in December, the first month of Summer enrollment, are 97.66% more likely to not enroll than a
student who sets their appointment in June. A student who sets their appointment in January is 144.72% more likely to not enroll than a
student who sets their appointment in June. A student who sets their appointment in February or March is 79.76% and 241.05% more likely to
not enroll, respectively, than a student who sets their appointment in June. Notice that April and May are not listed here because they
were not statistically significantly different than June. We can conclude that bookings appointments made during the months of April and
May have roughly the same likelihood of ending in enrollment as those made in June. 

#### Evaluate Student Type

There are three student types--Current, High School, and New Student. Current students are the reference level of this model.

High school students are 27.36% more likely to not enroll than a current student who sets a booking appointment. New Students are 109.34%
more likely to not enroll than a current student who sets a booking appointment. 

#### Evaluate The Follow Up Contacts

This is the big one you were interested in. Unequivocally, we can conclude that additional contacts **do not** increase the odds a student
will enroll. In fact, the odds of a student being *Not Enrolled* are 8.80 times higher with *each additional contact*, assuming all other
variables are held constant!! There is rarely a time where we can emphatically say, additional contacts have no statistically significant
effect on student enrollment. 